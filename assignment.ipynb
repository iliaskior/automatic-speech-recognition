{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yerWQD2Tzokl"
   },
   "source": [
    "## Import Modules and Prerequisites\n",
    "---\n",
    "Please use this section to import any necessary modules that will be required later in this notebook like the example given."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-30T08:02:19.275150Z",
     "start_time": "2019-08-30T08:02:09.413650Z"
    },
    "id": "GCx876Ilzokm"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# add any needed libraries\n",
    "from pathlib import Path\n",
    "from typing import Union\n",
    "import librosa\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "il8e-reUP7S_"
   },
   "source": [
    "## Automatic Speech Recognition\n",
    "---\n",
    "#### Note: There is no expectation of coding a highly sophisticated solution in this current small time period. Each question can be answered either with a short code example along with a possible written explaination of a more elaborate approach or with not highly tuned models, due to lack of available resources and time.\n",
    "\n",
    "A common task in Acoustics is to predict the speaker from corresponding audio signals (speaker identification). In the provided corpus (see the project description), you can find transcripts under various speech settings and speaking conditions. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7rNAWFkpJNOJ"
   },
   "source": [
    "### 1. Train a classifier on the Solo Speech condition dataset that will reach an acceptable accuracy score.\n",
    "---\n",
    "Feel free to follow any design choices you feel fit the problem best. Briefly describe your approach in markdown cells, along with any necessary comments on your choices. Explain your choices with the appropriate evaluation plots - analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "nRuD2Jc2Ie41"
   },
   "outputs": [],
   "source": [
    "#Path initialization\n",
    "ROOT_PATH = Path.cwd()\n",
    "ROOT_DATASET_PATH = ROOT_PATH.joinpath('dataset')\n",
    "SOLO_DATASET_PATH = ROOT_DATASET_PATH.joinpath('data').joinpath('solo')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a list which contains all the audio files' paths in solo dataset\n",
    "audiofiles = list((SOLO_DATASET_PATH).glob('**/*.wav'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def audio_load(filepath: Union[str, Path], sr: int) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Load an audio file as a floating point time series.\n",
    "    Audio will be automatically resampled to the given rate\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    filepath : Union[str, Path]\n",
    "        Local path of the audio file\n",
    "    sr : int\n",
    "        target sampling rate\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    np.ndarray\n",
    "        audio time series\n",
    "    \"\"\"\n",
    "\n",
    "    #Read audio file as floating point time series\n",
    "    try:\n",
    "        sig, sr = librosa.load(filepath, sr=sr)\n",
    "    except Exception as e:\n",
    "        print(f\"An error occured while processing {filepath}\")\n",
    "        print(e)\n",
    "        return None\n",
    "    return sig\n",
    "\n",
    "def get_duration(sig:np.ndarray, sr:int) -> float:\n",
    "    \"\"\"Returns the audio duration in seconds\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    sig : np.ndarray\n",
    "        Target signal\n",
    "    sr : int\n",
    "        The sampling rate used for audio loading\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    float\n",
    "        audio file duration\n",
    "    \"\"\"\n",
    "    return sig.shape[0]/sr\n",
    "\n",
    "def split_to_signals(sig: np.ndarray, size:int=4000, slide:int=4000) -> np.ndarray:\n",
    "    \"\"\"Cuts signal in segments of ``size`` length and ``slide`` overlap\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    signal : np.ndarray\n",
    "        Input Signal\n",
    "    size : int, optional\n",
    "        Size of block in number of samples, by default 4000\n",
    "    slide : int, optional\n",
    "        Slide increase in number of samples, by default 4000\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    np.ndarray\n",
    "        Two-dimentional array of signal segments\n",
    "    \"\"\"\n",
    "    \n",
    "    if slide is None:\n",
    "        slide = size\n",
    "\n",
    "    assert size >= slide, \"Size should be greater or equal than slide length\"\n",
    "\n",
    "    return librosa.util.frame(sig, frame_length=size, hop_length=slide, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(40117,)\n"
     ]
    }
   ],
   "source": [
    "sig = audio_load(audiofiles[0], sr=16000)\n",
    "print(sig.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 4000)\n"
     ]
    }
   ],
   "source": [
    "segs = split_to_signals(sig)\n",
    "print(segs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.5073125\n"
     ]
    }
   ],
   "source": [
    "dur = get_duration(sig, 16000)\n",
    "print(dur)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "97X2QamRrxw8"
   },
   "source": [
    "### 2. Assuming that you needed to apply the learned rules / models on the Fast Speech condition dataset, without having that (test) dataset beforehand, what you would do?\n",
    "---\n",
    "The goal is to approach the classification accuracy obtained on the train dataset to the test dataset, without using the latter for training. Describe any challenges (if they exist) and code your solution below following the same guidelines "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0JqQXHnvO3_v"
   },
   "source": [
    "### 3. Another important task is to perform gender classification on the same datasets, but there are no available labels. You can use the entirety of data you have at your disposal. Describe possible approaches to this problem and code the most robust solution of your choice. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pdSXQbB6IcyL"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v-9uwDOUzolE"
   },
   "source": [
    "## Thank you in advance. Good luck!"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "assignment.ipynb",
   "provenance": [
    {
     "file_id": "1zLg8KDovmYmXsc3HcpVRNyU-XouJY4C9",
     "timestamp": 1630683886599
    },
    {
     "file_id": "0B_k471A7UcJSUS1ZWTNsWWV4T21CanBvME9iTUlXaUQtQlQw",
     "timestamp": 1554849290453
    }
   ]
  },
  "kernelspec": {
   "display_name": "asr39",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  },
  "vscode": {
   "interpreter": {
    "hash": "2840966a3ecc76691f6bdd33fb23dcb12af21bbbc17346653c94e962c3cbfde1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
