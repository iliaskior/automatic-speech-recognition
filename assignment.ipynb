{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yerWQD2Tzokl"
   },
   "source": [
    "## Import Modules and Prerequisites\n",
    "---\n",
    "Please use this section to import any necessary modules that will be required later in this notebook like the example given."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-30T08:02:19.275150Z",
     "start_time": "2019-08-30T08:02:09.413650Z"
    },
    "id": "GCx876Ilzokm"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# add any needed libraries\n",
    "import os\n",
    "import sys\n",
    "import librosa\n",
    "import librosa.display\n",
    "from pathlib import Path\n",
    "import seaborn as sns\n",
    "sys.path.append(os.path.abspath(os.path.join('audio')))\n",
    "from audioblock import audio_load, extract_features, split_to_signals, get_duration\n",
    "\n",
    "sys.path.append(os.path.abspath(os.path.join('utils')))\n",
    "from utils import load_pickle, plot_confusion_matrix\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "il8e-reUP7S_"
   },
   "source": [
    "## Automatic Speech Recognition\n",
    "---\n",
    "#### Note: There is no expectation of coding a highly sophisticated solution in this current small time period. Each question can be answered either with a short code example along with a possible written explaination of a more elaborate approach or with not highly tuned models, due to lack of available resources and time.\n",
    "\n",
    "A common task in Acoustics is to predict the speaker from corresponding audio signals (speaker identification). In the provided corpus (see the project description), you can find transcripts under various speech settings and speaking conditions. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7rNAWFkpJNOJ"
   },
   "source": [
    "### 1. Train a classifier on the Solo Speech condition dataset that will reach an acceptable accuracy score.\n",
    "---\n",
    "Feel free to follow any design choices you feel fit the problem best. Briefly describe your approach in markdown cells, along with any necessary comments on your choices. Explain your choices with the appropriate evaluation plots - analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nRuD2Jc2Ie41"
   },
   "outputs": [],
   "source": [
    "#Path initialization\n",
    "ROOT_PATH = Path.cwd()\n",
    "ROOT_DATASET_PATH = ROOT_PATH.joinpath('dataset')\n",
    "SOLO_DATASET_PATH = ROOT_DATASET_PATH.joinpath('data').joinpath('solo')\n",
    "OUTPUT_PATH = ROOT_PATH.joinpath('output')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Audio Feature Set"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I going to use one kind of feature, known as MFCCs. The mel frequency cepstral coefficients (MFCCs) of an audio signal are a small set of features (usually about 10–20) which describe the overall shape of the spectral envelope. MFCCs were frequently used for voice recognition. It was also used to describe “timbre”.\n",
    "\n",
    "Furthermore, I will enrich the mfccs features with deltas and deltas-deltas MFCCs. The idea behind using delta (differential) and delta-delta (acceleration) coefficients is that in order to recognize speech better, we need to understand the dynamics of the power spectrum, i.e., the trajectories of MFCCs over time.\n",
    "\n",
    "It is common to use the 13 MFCCs for this kind of tasks, so and I will use 13 MFCCs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets extract the above mentioned features for an audio file of the SOLO dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a list which contains all the audio files' paths of solo dataset\n",
    "audiofiles_solo = list((SOLO_DATASET_PATH).glob('**/*.wav'))\n",
    "\n",
    "SAMPLE_RATE = 16000 #Hz\n",
    "NUM_MFCC = 13\n",
    "N_FFT = 128 #milliseconds\n",
    "HOP_LENGTH = 32 #Hz\n",
    "sig = audio_load(filepath=audiofiles_solo[0], sr=SAMPLE_RATE)\n",
    "\n",
    "mfccs, delta_mfccs, delta2_mfccs = extract_features(\n",
    "                    sig = sig,\n",
    "                    sr = SAMPLE_RATE,\n",
    "                    num_mfcc=NUM_MFCC,\n",
    "                    n_fft=N_FFT,\n",
    "                    hop_length=HOP_LENGTH\n",
    "                )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets plot the waveform and the extracted features mfccs, deltas and deltas-deltas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(nrows=4, ncols=1, sharex=True, figsize = (20, 15))\n",
    "\n",
    "librosa.display.waveshow(sig, sr=SAMPLE_RATE, ax=ax[0])  # put signal in row 0, \n",
    "librosa.display.specshow(mfccs, sr=SAMPLE_RATE ,x_axis='time', ax=ax[1]) # mfccs in row 1\n",
    "librosa.display.specshow(delta_mfccs, sr=SAMPLE_RATE ,x_axis='time', ax=ax[2]) # deltas mfccs in row 2\n",
    "librosa.display.specshow(delta2_mfccs, sr=SAMPLE_RATE ,x_axis='time', ax=ax[3]) # delta-deltas mfccs in row 3"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the selection of MFCCs as my feature set, I had to decide what kind of classifier to use. I could use trandiotinal machine learning algorithms such as KNN. Although, deep neural networks achieve highest accuracy, so I followed this path. \n",
    "\n",
    "I choose as classifier a DNN with LSTM layers. LSTM networks are well-suited to classifying, processing and making predictions based on time series data. Long short-term memory (LSTM) network has been proved to be effective with temporal features because they can handle the sequence of the features using a type of memory.\n",
    "\n",
    "LSTM expects the feature vectors to be in the same shape. This means that in order to use a LSTM architecture the feature extraction must occur using fixed size time windows of the audio files. \n",
    "\n",
    "However our audio files are not in the same length (duration). So I need to split each audio file in fixed time windows and extract the features for each segment. I choose to use 2 seconds time window without overlap.\n",
    "\n",
    "So lets split the above waveform into 2 second time windows and extract the features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get the audio file's duration\n",
    "duration = get_duration(sig, sr=SAMPLE_RATE)\n",
    "print(f\"The audio file's duration is {round(duration, 2)} seconds\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we expect only one segment after the signal's splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "WINDOW_SIZE = 2000 #milliseconds\n",
    "WINDOW_STEP = 2000 #milliseconds, this means that we dont have overlap\n",
    "\n",
    "segs = split_to_signals(sig, sr=SAMPLE_RATE, size=WINDOW_SIZE, slide=WINDOW_STEP)\n",
    "print(f\"The number of audio segments after signal splitting is: {len(segs)}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets extract the mfccs, deltas and delta-deltas features for the segment of 2 seconds, and concat them in order to see the shape of the feature vectors which describe the audio segment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_FFT = 128 #milliseconds\n",
    "HOP_LENGTH = 32 #milliseconds\n",
    "NUM_MFCC = 13\n",
    "\n",
    "mfccs, delta_mfccs, delta2_mfccs = extract_features(\n",
    "    sig = segs[0],\n",
    "    sr = SAMPLE_RATE,\n",
    "    num_mfcc=NUM_MFCC,\n",
    "    n_fft=N_FFT,\n",
    "    hop_length=HOP_LENGTH\n",
    ")\n",
    "\n",
    "#vertical feature stack\n",
    "mfccs_conc = np.vstack((mfccs, delta_mfccs))\n",
    "mfccs_conc = np.vstack((mfccs_conc, delta2_mfccs))\n",
    "print(mfccs_conc.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, as we can see then the time window is represented by 63 timesteps of 39 features."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can use the flows/audio_feature_extraction.py to extract the above mentioned audio features for fixed size time windows for a set of audio files"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setting up the experiment"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our target is to create a speaker indentification system using the extracted features from all the audio files of SOLO dataset. \n",
    "This means that we need to use some data from all the users in the training and use the rest of them in testing in order to see our system's evaluation.\n",
    "Altough in order out experiment to be valid, we must secure that segments of the same audio file will not be in the training and in the testing at the same time. \n",
    "\n",
    "Furthermore we want the evaluate our model with each audio file and not only with a small part of the dataset. \n",
    "For this reason I used a GroupKFold experimentation setup, with 5 folds without overlap. \n",
    "Each fold represents 20% of the initial dataset.\n",
    "The groups are created using the filename as a key. \n",
    "So, audio segments of the same audio file are always in the same fold, avoiding any type\n",
    "of data leakage between train and test set\n",
    "\n",
    "Furthermore, with GroupKFold, we secure that all the audio files will be in testing position at the end,\n",
    "because with each iteration 4 folds are used for training and the rest one for testing in a circular manner\n",
    "\n",
    "Finally, I wanted to have a validation set for monitoring the model training. I followed the same method\n",
    "and I used the above 4 folds for performing a second GroupKFold with only one iteration (5 splits, 4 for training and 1 for validation)\n",
    "\n",
    "The code for all these can be found in flows/train_speaker_identification.py\n",
    "\n",
    "In the below cell we can see the results of the method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_path = OUTPUT_PATH.joinpath('step1_results_groupkfold.pickle')\n",
    "results = load_pickle(results_path)\n",
    "results"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the confusion matrix (normalized and not normlized values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrix(results['label'].values, results['prediction'].values, norm=False, fullpath=OUTPUT_PATH.joinpath('step1_cm.jpg'))\n",
    "plot_confusion_matrix(results['label'].values, results['prediction'].values, norm=True, fullpath=OUTPUT_PATH.joinpath('step1_normalized_cm.jpg'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "97X2QamRrxw8"
   },
   "source": [
    "### 2. Assuming that you needed to apply the learned rules / models on the Fast Speech condition dataset, without having that (test) dataset beforehand, what you would do?\n",
    "---\n",
    "The goal is to approach the classification accuracy obtained on the train dataset to the test dataset, without using the latter for training. Describe any challenges (if they exist) and code your solution below following the same guidelines "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regarding this task, it is each very easy to guess that the pretrained model using the SOLO dataset will have a low performance predicting the FAST dataset. Let's confirm it. \n",
    "Steps:\n",
    " 1. Extract features for the FAST dataset using the flows/audio_feature_extraction.py\n",
    " 2. Use the pretrained model in SOLO dataset to predict the FAST dataset using flows/predict_speaker.py\n",
    "\n",
    "The results are the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_path = OUTPUT_PATH.joinpath('step2_results.pickle')\n",
    "results = load_pickle(results_path)\n",
    "results"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the confusion matrix (normalized and not normlized values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrix(results['label'].values, results['prediction'].values, norm=False, fullpath=OUTPUT_PATH.joinpath('step2_cm.jpg'))\n",
    "plot_confusion_matrix(results['label'].values, results['prediction'].values, norm=True, fullpath=OUTPUT_PATH.joinpath('step2_normalized_cm.jpg'))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The reason the model didn't achieve well is very clear. The pretrained model is an LSTM architecture that was trained to recognize the speaker who talks with a normal tempo. Talking very fast affects the sequence of MFCCs features and the model fails because expects the features with different sequences or in other words fewer phonemes in a fixed-size time window.\n",
    "\n",
    "So, to solve that problem we need to find a different solution in which time doesn't play a role.\n",
    "Such a solution would be to use the wav2vec 2.0,\n",
    "\n",
    "Wav2vec 2.0 is a framework that achieves state-of-art results in ASR tasks regardless of how fast a user speaks or the pauses in his talk \n",
    "The idea behind this is that since the model achieves very good results in speech-to-text problem it means that in some intermediate layer of it, the model captures the user's 'voice' in a perfect way.\n",
    "A wav2vec model consists of a feature encoder with 7 blocks of CNN layers followed by a transformer encoder with 24 blocks of layers\n",
    "So, we can exploit its potential, extracting audio representations from an intermediate layer as I did during my thesis (dealing with speech emotion recognition) achieving state-of-art results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0JqQXHnvO3_v"
   },
   "source": [
    "### 3. Another important task is to perform gender classification on the same datasets, but there are no available labels. You can use the entirety of data you have at your disposal. Describe possible approaches to this problem and code the most robust solution of your choice. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In general, women speak at a higher pitch—about an octave higher than men. An adult woman's average range is from 165 to 255 Hz, while a man's is 85 to 155 Hz.\n",
    "\n",
    "Due to the lack of gender labels I will try to classify the gender using the fundemental frequency of the corresponding audio file using a statistical rule. For the extraction of the fundamendal frequency I used an algorithm called Probabilistic YIN (PYIN) which is implemented by librosa library. \n",
    "\n",
    "PYIN is a modification of the YIN algorithm for the estimation of the fundamental frequency (F0) but with better results. The first stage of PYIN follows the same steps as the original YIN algorithm, differing only in the thresholding stage, where it assumes a threshold distribution, in contrast to YIN, which relies on a single threshold. \n",
    "\n",
    "The PYIN method is divided into two stages: \n",
    "- frame-wise extraction of multiple pitch candidates with associated probabilities \n",
    "- these probabilities are used  as emission probabilities in a Hidden Markov Model which trys to find the likely pitch sequence given the emissions\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets extract the fundamental frequency of an audio file from SOLO dataset.\n",
    "I will use the librosa's default parameters for PYIN algorithm but I will search for F0 between 50 and 300 Hz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAMPLE_RATE = 22050\n",
    "\n",
    "#Load audio file\n",
    "sig = audio_load(filepath=audiofiles_solo[0], sr=SAMPLE_RATE)\n",
    "\n",
    "#Apply the PYIN algorithm for extracting the F0 of the signal\n",
    "f0, voiced_flag, voiced_probs = librosa.pyin(\n",
    "    sig,\n",
    "    fmin=50,\n",
    "    fmax=300,\n",
    "    sr=SAMPLE_RATE\n",
    ")\n",
    "\n",
    "times = librosa.times_like(f0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overlay F0 over a spectrogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "D = librosa.amplitude_to_db(np.abs(librosa.stft(sig)), ref=np.max)\n",
    "fig, ax = plt.subplots()\n",
    "img = librosa.display.specshow(D, x_axis='time', y_axis='log', ax=ax)\n",
    "ax.set(title='pYIN fundamental frequency estimation')\n",
    "fig.colorbar(img, ax=ax, format=\"%+2.f dB\")\n",
    "ax.plot(times, f0, label='f0', color='cyan', linewidth=3)\n",
    "ax.legend(loc='upper right')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see from the plot the F0 values are about 200Hz which means that the voice belongs to a woman."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In general, for predicting the gender from fundamendal frequency we need to set a statistical rule. As I mentioned above about the pitch, an adult woman's average range is from 165 to 255 Hz, while a man's is 85 to 155 Hz. So we can set a threshold at 160 Hz. So I compared this threshold with the most representative value of F0, which is the median value of the vector. So:\n",
    "- If F0 > 160Hz then the speaker is a female\n",
    "- If F0 <= 160Hz then the speaker is a male\n",
    "\n",
    "Using the /flows/gender_classification.py, a dataset is created calculating the median and mean value of F0 for each audio file of the dataset.\n",
    "Also, I created a mapping dictionary between user name and its gender from dataset's description pdf file for evaluating my method.\n",
    "\n",
    "For this step, I used both SOLO and FAST dataset.\n",
    "\n",
    "Let's load the extracted dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = OUTPUT_PATH.joinpath('step3_dataset.pickle')\n",
    "dataset = load_pickle(dataset_path)\n",
    "dataset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets plot a grouped boxplot of median f0 distribution across male and female genders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(x='gender', y='median_f0', data=dataset)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above boxplot confirms that the rule I set will recognize the gender with high accuracy"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets apply the rule I set and see the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Predict gender based on rule\n",
    "threshold = 160 #Hz\n",
    "dataset[\"prediction\"] = dataset[\"median_f0\"].apply(lambda x: 'female' if x>threshold else 'male')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrix(dataset['gender'].values, dataset['prediction'].values, norm=False, fullpath=OUTPUT_PATH.joinpath('step3_cm.jpg'), figsize=(8,5))\n",
    "plot_confusion_matrix(dataset['gender'].values, dataset['prediction'].values, norm=True, fullpath=OUTPUT_PATH.joinpath('step3_normalized_cm.jpg'), figsize=(8,5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v-9uwDOUzolE"
   },
   "source": [
    "## Thank you in advance. Good luck!"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "assignment.ipynb",
   "provenance": [
    {
     "file_id": "1zLg8KDovmYmXsc3HcpVRNyU-XouJY4C9",
     "timestamp": 1630683886599
    },
    {
     "file_id": "0B_k471A7UcJSUS1ZWTNsWWV4T21CanBvME9iTUlXaUQtQlQw",
     "timestamp": 1554849290453
    }
   ]
  },
  "kernelspec": {
   "display_name": "brain_tumor_py39",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  },
  "vscode": {
   "interpreter": {
    "hash": "047cfb83cf53d8759eb3dcee9607eb42d46e6cd73ff22b9db25630cabfb0d3db"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
